{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"blue\">Lesson 6 - Feature Engineering and Selection</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Cross Validation? \n",
    "We've actually been using cross validation this entire time, when you've held out a percentage of your dataset for testing. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train, validation = train_test_split(data, \n",
    "                    test_size=0.50, \n",
    "                    random_state = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there are a few different approaches to cross validation that can help you more accurately measure model performance. Let's take a look at some of the most common "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave One Out\n",
    "With this method, we keep out just one data point from out dataset and train the model on the rest of the data. We repeat this process for each data point. \n",
    "\n",
    "- Use all data points, so bias will be low\n",
    "- Repeat cross validation process n times, resulting in longer execution time\n",
    "- Approach leads to higher variation in testing model effectiveness because we test against one data point. So, estimation gets highly influenced by the data point. If the data point turns out to be an outlier, it can lead to a higher variation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "# create fake training data\n",
    "X = np.array([[1, 2], [3, 4]])\n",
    "y = np.array([1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "# determine how many times to split\n",
    "loo.get_n_splits(X)\n",
    "\n",
    "# apply leave one out for each data point\n",
    "for train_index, test_index in loo.split(X):\n",
    "        print(\"train:\", train_index, \"validation:\", test_index)\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold \n",
    "This is one of the most common validation methods. \n",
    "\n",
    "1. Randomly split dataset into k”folds”\n",
    "2. For each k-fold in your dataset, build your model on k – 1 folds of the dataset. \n",
    "3. Test the model with the left out fold\n",
    "4. Record the error you see on each of the predictions\n",
    "5. Repeat this until each of the k-folds has served as the test set\n",
    "6. Calc the cross-validation error, which is the average of your k recorded errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# create a dataset\n",
    "X = [\"a\", \"b\", \"c\", \"d\"]\n",
    "\n",
    "# instantiate kfold model\n",
    "kf = KFold(n_splits=2)\n",
    "\n",
    "# split the dataset into k folds and view indices\n",
    "for train, test in kf.split(X):\n",
    "     print(\"%s %s\" % (train, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each fold is constituted by two arrays: the first one is related to the training set, and the second one to the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Consider this\n",
    "Which method is more robust to an outlier? Re-run this simulation with an obvious outlier in your X array."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
