{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"reminder\" style=\"border-radius: 5px; background-color:#f5f5f5; padding: 15px 5px; \" >\n",
    "<p>Use this notebook to follow along with the lab tutorial.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"blue\">Lesson 8 SVM and Neural Nets</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include Functions, Load Data and set Gamma and Cost Hyperparameters\n",
    "\n",
    "Setting hyperparameters at the top of the file gives you a way to find them and change them in a single place.\n",
    "\n",
    "Iris is a classical dataset for doing classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.843333</td>\n",
       "      <td>3.057333</td>\n",
       "      <td>3.758000</td>\n",
       "      <td>1.199333</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.828066</td>\n",
       "      <td>0.435866</td>\n",
       "      <td>1.765298</td>\n",
       "      <td>0.762238</td>\n",
       "      <td>0.819232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.300000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.100000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.400000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sepal length (cm)  sepal width (cm)  petal length (cm)  \\\n",
       "count         150.000000        150.000000         150.000000   \n",
       "mean            5.843333          3.057333           3.758000   \n",
       "std             0.828066          0.435866           1.765298   \n",
       "min             4.300000          2.000000           1.000000   \n",
       "25%             5.100000          2.800000           1.600000   \n",
       "50%             5.800000          3.000000           4.350000   \n",
       "75%             6.400000          3.300000           5.100000   \n",
       "max             7.900000          4.400000           6.900000   \n",
       "\n",
       "       petal width (cm)      target  \n",
       "count        150.000000  150.000000  \n",
       "mean           1.199333    1.000000  \n",
       "std            0.762238    0.819232  \n",
       "min            0.100000    0.000000  \n",
       "25%            0.300000    0.000000  \n",
       "50%            1.300000    1.000000  \n",
       "75%            1.800000    2.000000  \n",
       "max            2.500000    2.000000  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris() # Load the iris dataset\n",
    "\n",
    "# convert to a dataframe\n",
    "tdf = pd.DataFrame(data= np.c_[iris['data'], iris['target']], columns= iris['feature_names'] + ['target'])\n",
    "\n",
    "tdf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate into data and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate Features from Target\n",
    "X = tdf.iloc[:, 0:4]   # load features into X DF\n",
    "Y = tdf.iloc[:, 4]     # Load target into Y DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split into a training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the hyperparameters \n",
    "Having this in one location enables finding them more easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = .9 # penalty parameter of the error term\n",
    "gamma = 5 # defines the influence of input vectors on the margins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Kernels\n",
    "I'm creating a for loop so that I can test multiple kernels: polynomial, radial basis function and linear kernels against a linearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00         8\n",
      "         1.0       0.73      0.89      0.80         9\n",
      "         2.0       0.91      0.77      0.83        13\n",
      "\n",
      "    accuracy                           0.87        30\n",
      "   macro avg       0.88      0.89      0.88        30\n",
      "weighted avg       0.88      0.87      0.87        30\n",
      "\n",
      "linear\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00         8\n",
      "         1.0       0.82      1.00      0.90         9\n",
      "         2.0       1.00      0.85      0.92        13\n",
      "\n",
      "    accuracy                           0.93        30\n",
      "   macro avg       0.94      0.95      0.94        30\n",
      "weighted avg       0.95      0.93      0.93        30\n",
      "\n",
      "rbf\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00         8\n",
      "         1.0       0.82      0.90      0.86        10\n",
      "         2.0       0.91      0.83      0.87        12\n",
      "\n",
      "    accuracy                           0.90        30\n",
      "   macro avg       0.91      0.91      0.91        30\n",
      "weighted avg       0.90      0.90      0.90        30\n",
      "\n",
      "poly\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00         8\n",
      "         1.0       1.00      0.85      0.92        13\n",
      "         2.0       0.82      1.00      0.90         9\n",
      "\n",
      "    accuracy                           0.93        30\n",
      "   macro avg       0.94      0.95      0.94        30\n",
      "weighted avg       0.95      0.93      0.93        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm, metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Test a LinearSVC\n",
    "clf1 = svm.LinearSVC(C=cost).fit(X_train, y_train)\n",
    "clf1.predict(X_test)\n",
    "print(\"LinearSVC\")\n",
    "print(classification_report(clf1.predict(X_test), y_test))\n",
    "\n",
    "# Test linear, rbf and poly kernels\n",
    "for k in ('linear', 'rbf', 'poly'):\n",
    "    clf = svm.SVC(gamma=gamma, kernel=k, C=cost).fit(X_train, y_train)\n",
    "    clf.predict(X_test)\n",
    "    print(k)\n",
    "    print(classification_report(clf.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Different Kernels with 2 Features\n",
    "In order to simplify the data visualization, I'm using only 2 features at a time. We can see because of the use of cost and gamma we've made the margins soft. \n",
    "\n",
    "First we'll plot features 1 and 2 and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload iris data\n",
    "(data, target) = load_iris(return_X_y =True)\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make plotting easier, let's just use two features.\n",
    "X = data[:,:2]\n",
    "Y = target\n",
    "\n",
    "h = .5  # step size in the mesh\n",
    "#cost = .9  # update the cost\n",
    "#gamma = 10 # update the gamma \n",
    "\n",
    "# testing other kernels on unscaled data (for plotting tht support vectors)\n",
    "svc = svm.SVC(kernel='linear', C=cost).fit(X, Y)\n",
    "rbf_svc = svm.SVC(kernel='rbf', gamma=gamma, C=cost).fit(X, Y)\n",
    "poly_svc = svm.SVC(kernel='poly', gamma=gamma, degree=3, C=cost).fit(X, Y)\n",
    "lin_svc = svm.LinearSVC(C=cost).fit(X, Y)\n",
    "\n",
    "# create a mesh to plot in\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "# title for the plots\n",
    "titles = ['SVC with linear kernel',\n",
    "          'SVC with RBF kernel',\n",
    "          'SVC with polynomial kernel',\n",
    "          'LinearSVC (linear kernel)']\n",
    "\n",
    "for i, kernel in enumerate((svc, rbf_svc, poly_svc, lin_svc)):\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    Z = kernel.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    \n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Plot also the training points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired)\n",
    "    plt.title(titles[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features one and two are not very good at creating separation\n",
    "\n",
    "Let's try features 2 and 3 and plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = data[:, 1:3]\n",
    "\n",
    "# testing other kernels on unscaled data (for plotting tht support vectors)\n",
    "svc1 = svm.SVC(kernel='linear', C=cost).fit(X1, Y)\n",
    "rbf_svc1 = svm.SVC(kernel='rbf', gamma=gamma, C=cost).fit(X1, Y)\n",
    "poly_svc1 = svm.SVC(kernel='poly', degree=3, C=cost).fit(X1, Y)\n",
    "lin_svc1 = svm.LinearSVC(C=cost).fit(X1, Y)\n",
    "\n",
    "x1_min, x1_max = X1[:, 0].min() - 1, X1[:, 0].max() + 1\n",
    "y1_min, y1_max = X1[:, 1].min() - 1, X1[:, 1].max() + 1\n",
    "xx1, yy1 = np.meshgrid(np.arange(x1_min, x1_max, h),\n",
    "                     np.arange(y1_min, y1_max, h))\n",
    "\n",
    "for i, kernel1 in enumerate((svc1, rbf_svc1, poly_svc1, lin_svc1)):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    Z1 = kernel1.predict(np.c_[xx1.ravel(), yy1.ravel()])\n",
    "    Z1 = Z1.reshape(xx1.shape)\n",
    "    plt.contourf(xx1, yy1, Z1)\n",
    "    plt.axis('off')\n",
    "    plt.scatter(X1[:, 0], X1[:, 1], c=Y, cmap=plt.cm.Paired)\n",
    "    plt.title(titles[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features 2 and 3 are better than features 1 and 2. But how will they compare with 3 and 4?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = data[:, 2:4]\n",
    "\n",
    "# testing other kernels on unscaled data (for plotting tht support vectors)\n",
    "svc2 = svm.SVC(kernel='linear', C=cost).fit(X2, Y)\n",
    "rbf_svc2 = svm.SVC(kernel='rbf', gamma=gamma, C=cost).fit(X2, Y)\n",
    "poly_svc2 = svm.SVC(kernel='poly', C=cost).fit(X2, Y)\n",
    "lin_svc2 = svm.LinearSVC(C=cost).fit(X2, Y)\n",
    "\n",
    "x2_min, x2_max = X2[:, 0].min() - 1, X2[:, 0].max() + 1\n",
    "y2_min, y2_max = X2[:, 1].min() - 1, X2[:, 1].max() + 1\n",
    "xx2, yy2 = np.meshgrid(np.arange(x2_min, x2_max, h),\n",
    "                     np.arange(y2_min, y2_max, h))\n",
    "\n",
    "for i, kernel2 in enumerate((svc2, rbf_svc2, poly_svc2, lin_svc2)):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    Z2 = kernel2.predict(np.c_[xx2.ravel(), yy2.ravel()])\n",
    "    Z2 = Z2.reshape(xx2.shape)\n",
    "    plt.contourf(xx2, yy2, Z2)\n",
    "    plt.axis('off')\n",
    "    plt.scatter(X2[:, 0], X2[:, 1], c=Y, cmap=plt.cm.Paired)\n",
    "    plt.title(titles[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These look even better--but all of them is best!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"reminder\" style=\"border-radius: 5px; background-color:#f5f5f5; padding: 15px 5px; \" >\n",
    "<p>Please see the Simple Perceptron notebook for continued instruction.</p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
