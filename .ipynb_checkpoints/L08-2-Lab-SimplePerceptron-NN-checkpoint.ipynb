{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"reminder\" style=\"border-radius: 5px; background-color:#f5f5f5; padding: 15px 5px; \" >\n",
    "<p>Use this notebook to follow along with the lab tutorial.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"blue\">Lesson 8 SVM and Neural Nets</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments with a Perceptron Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Set\n",
    "For this simple example, we will use a handcrafted dataset. This dataset is a 6x2 (two features with six data points) associated with one output vector. It is crafted such that we have two arrays where numbers above 3 indicate activation (belong to class 1) and numbers below 3 indicate no activation (belong to class 0). 0.75 is a \"bias\" factor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.36316069 2.63794446 2.94467955 3.28708121 3.66929667 4.0959554 ]\n",
      " [2.69123447 2.80106583 2.9153795  3.03435839 3.15819291 3.28708121]]\n",
      "[0 0 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create an artificial dataset\n",
    "x1 = np.array(np.arange(0.1,0.7,0.1))\n",
    "X1 = np.exp(x1 * 1.1 + 0.75)\n",
    "x2 = np.array(np.arange(0.6,1.2,0.1))\n",
    "X2 = np.exp(x2 * 0.4 + 0.75)\n",
    "\n",
    "#From the output, lets use 3 as threshold; Value>3 = class 1, value<3 = class 0\n",
    "X = np.array([X1,X2])\n",
    "Y = np.array([0,0,0,1,1,1])\n",
    "\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic (Sigmoid) Function\n",
    "Exponential values for moderately large numbers tend to overflow. So np.clip is used here to limit the values of the signal between -500 and 500. Since e^x is between 0 and 1, the error in using this clip is low. Additionally, I am using logistic (sigmoid) function $\\frac{1}{1+e^-z}$. This can also be expressed as $\\frac{e^z}{1+e^z}$. NOTE: you could call sklearn.linear_model.LogisticRegressionCV(), but it's always good to try and write it yourself so you understand what the function does. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a numerically stable logistic s-shaped definition to call\n",
    "def sigmoid(x):\n",
    "    x = np.clip(x, -500, 500)\n",
    "    if x.any()>=0:\n",
    "        return 1/(1 + np.exp(-x))\n",
    "    else:\n",
    "        return np.exp(x)/(1 + np.exp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Parameters\n",
    "Because this there are not hidden layers, the second dimension is always assigned to 1. std is set to ${1^{-1}}$ to ensure values are between zero and 1. If zeros, there's no reason to multiply with std."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the dimentions and set the weights to random numbers\n",
    "def init_parameters(dim1, dim2=1,std=1e-1, random = True):\n",
    "    if(random):\n",
    "        return(np.random.random([dim1,dim2])*std)\n",
    "    else:\n",
    "        return(np.zeros([dim1,dim2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation\n",
    "Here, I am assuming a single layered network. Note that event with single layered network, the layer itself can have multiple nodes. Also, I am using vectorized operations here i.e not using explicit loops. This helps in processing multiple inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single layer network: Forward Prop\n",
    "# Passed in the weight vectors, bias vector, the input vector and the Y\n",
    "def fwd_prop(W1,bias,X,Y):\n",
    "\n",
    "    Z1 = np.dot(W1,X) + bias # dot product of the weights and X + bias\n",
    "    A1 = sigmoid(Z1)  # Uses sigmoid to create a predicted vector\n",
    "\n",
    "    return(A1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am calculating the loss/cost. The loss function here is a logistic loss function and in this case of binary classification, this is also a cross-entropy loss\n",
    "\n",
    "Cross Entropy loss for a single datapoint = $\\sum_{i=1}^{c} y_i*log (\\hat y_i) $\n",
    "For binary classification: $y_i*log (\\hat y_i) + (1-y_i)*log(1-\\hat y_i) $\n",
    "\n",
    "Lastly, the gradients W1 and B1 are calculated and returned along with the total cost/loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Single layer network: Backprop\n",
    "\n",
    "def back_prop(A1,W1,bias,X,Y):\n",
    "\n",
    "    m = np.shape(X)[1] # used the calculate the cost by the number of inputs -1/m\n",
    "   \n",
    "    # Cross entropy loss function\n",
    "    cost = (-1/m)*np.sum(Y*np.log(A1) + (1-Y)*np.log(1-A1)) # cost of error\n",
    "    dZ1 = A1 - Y                                            # subtract actual from pred weights\n",
    "    dW1 = (1/m) * np.dot(dZ1, X.T)                          # calc new weight vector\n",
    "    dBias = (1/m) * np.sum(dZ1, axis = 1, keepdims = True)  # calc new bias vector\n",
    "    \n",
    "    grads ={\"dW1\": dW1, \"dB1\":dBias} # Weight and bias vectors after backprop\n",
    "    \n",
    "    return(grads,cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "This function performs a simple gradient descent. After completing a round of forward propagation and backward propagation, the weights are updated based on the learning rate and gradient. The loss for that iteration is recorded in the loss_array. The function returns the final parameters W1 (updated weight vector), B1 (bias) and the loss array after running for given number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_grad_desc(num_epochs,learning_rate,X,Y,n_1):\n",
    "    \n",
    "    n_0, m = np.shape(X)\n",
    "    \n",
    "    W1 = init_parameters(n_1, n_0, True)\n",
    "    B1 = init_parameters(n_1,1, True)\n",
    "    \n",
    "    loss_array = np.ones([num_epochs])*np.nan # resets the loss_array to NaNs\n",
    "    \n",
    "    for i in np.arange(num_epochs):\n",
    "        A1 = fwd_prop(W1,B1,X,Y)                # get predicted vector\n",
    "        grads,cost = back_prop(A1,W1,B1,X,Y)    # get gradient and the cost from BP \n",
    "        \n",
    "        W1 = W1 - learning_rate*grads[\"dW1\"]    # update weight vector LR*gradient*[BP weights]\n",
    "        B1 = B1 - learning_rate*grads[\"dB1\"]    # update bias LR*gradient[BP bias]\n",
    "        \n",
    "        loss_array[i] = cost                    # loss array gets cross ent values\n",
    "        \n",
    "        parameter = {\"W1\":W1,\"B1\":B1}           # assign \n",
    "    \n",
    "    return(parameter,loss_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Experiment\n",
    "Now that all of the helper functions are created we can run gradient descent on the handcrafted dataset that I had created earlier. Note that I am using n_1 = 1, therefore, I am just using one output node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "params, loss_array = run_grad_desc(num_epochs,learning_rate,X,Y,n_1= 1 )\n",
    "print(loss_array[num_epochs-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the boundary of separation is 0. That is values less than 0 belong to class 0 and greater than 0 belong to class 1.\n",
    "Key thing to note here is that the data we generated was a linearly separable data and hence there are many possible options for the separting plane. Unlike SVM, logistic regression does not necessarily find the best separting plane, but finds a locally optimum solution that separates the classes of data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of the loss array\n",
    "Here we want to ensure that the loss value per iteration is going down. However, note that the plot has not curved to reach stablizing value i.e we can run the algorithms more times to get a lower loss. However, this is not needed as the current value of the parameters can classify the input data accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (8.0, 6.0) #Set default plot sizes\n",
    "plt.rcParams['image.interpolation'] = 'nearest' #Use nearest neighbor for rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_array)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another example with handcrafted dataset\n",
    "values below 0.5 are assigned to class 1 and values above are set to class 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = np.array([[0.25,0.75],[0.1,0.9],[0.3,0.8],[0.8,0.25],[0.9,0.2],[0.7,0.1]])\n",
    "X = np.array([[0.11,0.12],[0.05,0.1],[0.15,0.11],[0.8,0.9],[0.9,0.8],[0.85,0.95]])\n",
    "X = X.T #Had to do this because, I did not declare the X array as (#dimension * # Datapoints)\n",
    "Y = np.array([1,1,1,0,0,0])\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params, loss_array = run_grad_desc(100000,0.01,X,Y,n_1= 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_array)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things to Consider\n",
    "1. How would you stop this perceptron from overfitting?\n",
    "2. How would you convert this to a multi-layer Neural Network for a complex problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"reminder\" style=\"border-radius: 5px; background-color:#f5f5f5; padding: 15px 5px; \" >\n",
    "<p>Please see the Abalone SVC SVR notebook for your opportunity to try yourself.</p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
