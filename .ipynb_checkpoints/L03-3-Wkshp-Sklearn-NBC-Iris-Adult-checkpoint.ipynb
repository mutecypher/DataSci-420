{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"blue\">Lesson 3 - Basic Machine Learning Models</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NBC on Iris Data\n",
    "\n",
    "For this lesson, we'll use [sklearn's Iris flower dataset](http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html); this dataset contains measurements for three classes of Iris, with 50 oberservations per class. \n",
    "\n",
    "The following four measurements were taken for each of the three classes: \n",
    "- sepal length in cm\n",
    "- sepal width in cm\n",
    "- petal length in cm\n",
    "- petal width in cm\n",
    "- class = which of the three Iris classes this flower belongs to\n",
    "\n",
    "We can use the features to train our dataset, and use the class atribute as targets to measure our ground truth. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset and Packages\n",
    "We can load this dataset directly from sklearn: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "\n",
    "# load iris dataset\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use built-in sklearn functionality to pull apart the features and the targets. On a non-sklearn dataset, we would separate the features and targets manually by specifying column names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data gives us access to the features we can use to classify each sample\n",
    "features = iris.data\n",
    "targets = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this gives us arrays of measurments and targets to use for training our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.1, 3.5, 1.4, 0.2])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the dataset into training and test sets\n",
    "In order to test how effective we were at training our model on our dataset, we'll need to set aside a portion of our features and targets to test on; the industry standard is usually somewhere around 80/20 or 70/30. \n",
    "\n",
    "We'll do this using sklearn's [train_test_split feature](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html). Let's use the following arguments: \n",
    "- test_size = hold aside 30% of the features and targets for testing\n",
    "- random_state = set the seed used by the random number generator so that you can get the same results each time you run this same example\n",
    "- shuffle = Whether or not to shuffle the data before splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# splitting the dataset into 80/20 with random shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, targets, \n",
    "                                                    test_size=0.30, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check our feature training and test sets to see if the distribution of samples looks around 80/20:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Naive Bayes Model\n",
    "Now that we've prepared our dataset by pulling out the targets and features, and separating it into training and testing sets, we can train our model with the training features and targets, X_train, X_test. \n",
    "\n",
    "We'll use the [GaussianNB model from sklearn](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html) to first create our model. In our model, priors are learned from the data as we train. If we already know our priors, we could feed it in as a list using the \"priors\" option. \n",
    "\n",
    "### Create the Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train gaussian naive bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the Model\n",
    "Now that we have setup our model, we can \"fit\" it to our data, a process called \"training\". We only train our model on the training portion of the data and targets, never use any of the data that you've held aside for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit the model on your training set\n",
    "gnb_model = gnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've trained the Gaussian model, you can access the priors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb_model.class_prior_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions\n",
    "We've instantiated a model and trained it on our iris dataset. Now we can use this model to make predictions. \n",
    "\n",
    "Our model comes with a predict() method that we use on our testing features, X_test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make a prediction from the trained model on your test set\n",
    "predictions = gnb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Accuracy\n",
    "We've used our model to make predictions, so now we finally test how accurate those predictions are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, predictions, normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad for our first try; our model was 97% accurate! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NBC on Adult Income Data\n",
    "Now it's your turn to try a Naive Bayes classifier. For this portion of the lab, we'll be using the Adult Income Dataset that contains income data from about 32,000 adults. We're going to use these measurments to try and predict whether or not someone's income will be over, or under, $50k/year.  \n",
    "\n",
    "We'll walk you through importing and pre-processing the data, and then let you try training and testing your model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing data and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "s = requests.get(url).content\n",
    "adult_df = pd.read_csv(io.StringIO(s.decode('utf-8')), header=None)\n",
    "adult_df.columns = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\", \"marital_status\", \\\n",
    "                \"occupation\", \"relationship\", \"race\", \"sex\", \"capital_gain\", \"capital_loss\", \\\n",
    "               \"hours_per_week\", \"native_country\", \"income\"]\n",
    "adult_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data contains categorical variables, which we'll pull out and encode by changing to numeric values that can be used in our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_cols = adult_df.select_dtypes(include=[\"object\"]).columns\n",
    "obj_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the step above, we created a dataframe that contains just our categorical variables.  \n",
    "\n",
    "Now we can use pandas get_dummies() method to one-hot-encode these categorical variables into numeric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encode your dataframe\n",
    "data = pd.get_dummies(adult_df, columns=obj_cols, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull out Features and Targets\n",
    "Our data is now ready to be split into features and targets (income)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = data.drop('income_ >50K', axis=1)\n",
    "targets = data['income_ >50K']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training and test sets\n",
    "Just as we did above, we'll split our dataset into training and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, \n",
    "                                                    targets, \n",
    "                                                    test_size=0.20,\n",
    "                                                   random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Dataset\n",
    "Because the measurments in this dataset are not on the same scale, we need to adjust it so that it has a mean of 0 and a variance of 1. We'll use sklearn's StandardScaler. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.preprocessing as preprocessing\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement NBC Classifier\n",
    "Now that we've imported and processed the adult learning dataset, implement a Naive Bayes Classifer on this dataset, and asses the accuracy of your model, just like we did above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consider this\n",
    "How do you think we could improve the accuracy of this model? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
