{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 9 Assignment - Wine Neural Network\n",
    "\n",
    "   ## Author - Mike Pearson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "For this assignment you will start from the perceptron neural network notebook (Simple Perceptron Neural Network.ipynb) and modify the python code to make it into a multi-layer neural network. To test your system, use the RedWhiteWine.csv file with the goal of building a red or white wine classifier. Use all the features in the dataset, allowing the network to decide how to build the internal weighting system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "1. Use the provided RedWhiteWine.csv file. Include ALL the features with “Class” being your output vector\n",
    "2. Use the provided Simple Perceptron Neural Network notebook (copied below) to develop a multi-layer feed-forward/backpropagation neural network\n",
    "4. Be able to adjust the following between experiments:\n",
    "<ul>\n",
    "<li>Learning Rate\n",
    "<li>Number of epochs\n",
    "<li>Depth of architecture—number of hidden layers between the input and output layers\n",
    "<li>Number of nodes in a hidden layer—width of the hidden layers\n",
    "<li>(optional) Momentum\n",
    "    </ul>\n",
    "5. Determine what the best neural network structure and hyperparameter settings results in the\n",
    "best predictive capability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Set\n",
    "URL = \"https://library.startlearninglabs.uw.edu/DATASCI420/Datasets/RedWhiteWine.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments with a Perceptron Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 6)\n",
      "(6,)\n",
      "[[ 0.93638574 -0.01390486  0.9561014  -0.50135749]\n",
      " [ 0.96552706 -0.16839385  0.3517907   0.7301472 ]\n",
      " [-0.77429662 -0.82294929  0.64322454  0.52546871]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "wine_data = pd.read_csv(URL)\n",
    "##print(wine_data.head())\n",
    "##print(wine_data.dtypes)\n",
    "##print(wine_data.describe())\n",
    "yn0 = 2*np.random.random((3,4)) - 1\n",
    "\n",
    "\n",
    "# Create an artificial dataset\n",
    "randy = 0.6/6497\n",
    "x1 = np.array(np.arange(0.1,0.7,.1))\n",
    "X1 = np.exp(x1 * 1.1 + 0.75)\n",
    "x2 = np.array(np.arange(0.6,1.2,.1))\n",
    "X2 = np.exp(x2 * 0.4 + 0.75)\n",
    "\n",
    "#From the output, lets use 3 as threshold; Value>3 = class 1, value<3 = class 0\n",
    "X = np.array([X1,X2])\n",
    "Y = np.array([0,0,0,1,1,1])\n",
    "##Y = np.random.randint(0, 10, 6497)\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(yn0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic (Sigmoid) Function\n",
    "Exponential values for moderately large numbers tend to overflow. So np.clip is used here to limit the values of the signal between -500 and 500. Since e^x is between 0 and 1, the error in using this clip is low. Additionally, I am using logistic (sigmoid) function $\\frac{1}{1+e^-z}$. This can also be expressed as $\\frac{e^z}{1+e^z}$. NOTE: you could call sklearn.linear_model.LogisticRegressionCV(), but it's always good to try and write it yourself so you understand what the function does. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a numerically stable logistic s-shaped definition to call\n",
    "def sigmoid(x):\n",
    "    x = np.clip(x, -500, 500)\n",
    "    if x.any()>=0:\n",
    "        return 1/(1 + np.exp(-x))\n",
    "    else:\n",
    "        return np.exp(x)/(1 + np.exp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Parameters\n",
    "Because this there are not hidden layers, the second dimension is always assigned to 1. std is set to ${1^{-1}}$ to ensure values are between zero and 1. If zeros, there's no reason to multiply with std."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the dimentions and set the weights to random numbers\n",
    "def init_parameters(dim1, dim2=12,std=1e-1, random = True):\n",
    "    if(random):\n",
    "        return(np.random.random([dim1,dim2])*std)\n",
    "    else:\n",
    "        return(np.zeros([dim1,dim2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation\n",
    "Here, I am assuming a single layered network. Note that event with single layered network, the layer itself can have multiple nodes. Also, I am using vectorized operations here i.e not using explicit loops. This helps in processing multiple inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single layer network: Forward Prop\n",
    "# Passed in the weight vectors, bias vector, the input vector and the Y\n",
    "def fwd_prop(W1,bias,X,Y):\n",
    "\n",
    "    Z1 = np.dot(W1,X) + bias # dot product of the weights and X + bias\n",
    "    A1 = sigmoid(Z1)  # Uses sigmoid to create a predicted vector\n",
    "\n",
    "    return(A1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am calculating the loss/cost. The loss function here is a logistic loss function and in this case of binary classification, this is also a cross-entropy loss\n",
    "\n",
    "Cross Entropy loss for a single datapoint = $\\sum_{i=1}^{c} y_i*log (\\hat y_i) $\n",
    "For binary classification: $y_i*log (\\hat y_i) + (1-y_i)*log(1-\\hat y_i) $\n",
    "\n",
    "Lastly, the gradients W1 and B1 are calculated and returned along with the total cost/loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Single layer network: Backprop\n",
    "\n",
    "def back_prop(A1,W1,bias,X,Y):\n",
    "\n",
    "    m = np.shape(X)[1] # used the calculate the cost by the number of inputs -1/m\n",
    "   \n",
    "    # Cross entropy loss function\n",
    "    cost = (-1/m)*np.sum(Y*np.log(A1) + (1-Y)*np.log(1-A1)) # cost of error\n",
    "    dZ1 = A1 - Y                                            # subtract actual from pred weights\n",
    "    dW1 = (1/m) * np.dot(dZ1, X.T)                          # calc new weight vector\n",
    "    dBias = (1/m) * np.sum(dZ1, axis = 1, keepdims = True)  # calc new bias vector\n",
    "    \n",
    "    grads ={\"dW1\": dW1, \"dB1\":dBias} # Weight and bias vectors after backprop\n",
    "    \n",
    "    return(grads,cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "This function performs a simple gradient descent. After completing a round of forward propagation and backward propagation, the weights are updated based on the learning rate and gradient. The loss for that iteration is recorded in the loss_array. The function returns the final parameters W1 (updated weight vector), B1 (bias) and the loss array after running for given number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_grad_desc(num_epochs,learning_rate,X,Y,n_1):\n",
    "    \n",
    "    n_0, m = np.shape(X)\n",
    "    \n",
    "    W1 = init_parameters(n_1, n_0, True)\n",
    "    B1 = init_parameters(n_1,1, True)\n",
    "    \n",
    "    loss_array = np.ones([num_epochs])*np.nan # resets the loss_array to NaNs\n",
    "    \n",
    "    for i in np.arange(num_epochs):\n",
    "        A1 = fwd_prop(W1,B1,X,Y)                # get predicted vector\n",
    "        grads,cost = back_prop(A1,W1,B1,X,Y)    # get gradient and the cost from BP \n",
    "        \n",
    "        W1 = W1 - learning_rate*grads[\"dW1\"]    # update weight vector LR*gradient*[BP weights]\n",
    "        B1 = B1 - learning_rate*grads[\"dB1\"]    # update bias LR*gradient[BP bias]\n",
    "        \n",
    "        loss_array[i] = cost                    # loss array gets cross ent values\n",
    "        \n",
    "        parameter = {\"W1\":W1,\"B1\":B1}           # assign \n",
    "    \n",
    "    return(parameter,loss_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arrange the Wine data, scale the features\n",
    "\n",
    "Now that all of the helper functions are created we can run gradient descent on the handcrafted dataset that I had created earlier. Note that I am using n_1 = 1, therefore, I am just using one output node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       fixed acidity  volatile acidity   citric acid  residual sugar  \\\n",
      "count   6.497000e+03      6.497000e+03  6.497000e+03    6.497000e+03   \n",
      "mean    5.608211e-16     -3.067277e-15  4.212037e-15   -1.136774e-16   \n",
      "std     1.071433e-01      1.097576e-01  8.754088e-02    7.297245e-02   \n",
      "min    -2.822568e-01     -1.731107e-01 -1.919477e-01   -7.428275e-02   \n",
      "25%    -6.738075e-02     -7.311067e-02 -4.134531e-02   -5.587784e-02   \n",
      "50%    -1.779397e-02     -3.311067e-02 -5.200732e-03   -3.747293e-02   \n",
      "75%     4.005727e-02      4.022267e-02  4.299204e-02    4.074792e-02   \n",
      "max     7.177432e-01      8.268893e-01  8.080523e-01    9.257172e-01   \n",
      "\n",
      "          chlorides  free sulfur dioxide  total sulfur dioxide       density  \\\n",
      "count  6.497000e+03         6.497000e+03          6.497000e+03  6.497000e+03   \n",
      "mean   1.114006e-15        -8.894056e-17         -1.392275e-16  1.260695e-13   \n",
      "std    5.819535e-02         6.162986e-02          1.302347e-01  5.781132e-02   \n",
      "min   -7.812934e-02        -1.025185e-01         -2.528677e-01 -1.462625e-01   \n",
      "25%   -2.995658e-02        -4.696291e-02         -8.927321e-02 -4.543347e-02   \n",
      "50%   -1.500641e-02        -5.296248e-03          5.196833e-03  3.727900e-03   \n",
      "75%    1.489392e-02         3.637042e-02          9.275444e-02  4.421373e-02   \n",
      "max    9.218707e-01         8.974815e-01          7.471323e-01  8.537375e-01   \n",
      "\n",
      "                 pH     sulphates       alcohol       quality  \n",
      "count  6.497000e+03  6.497000e+03  6.497000e+03  6.497000e+03  \n",
      "mean  -4.271830e-15 -1.624835e-16 -3.264986e-15 -3.017372e-16  \n",
      "std    1.246412e-01  8.359881e-02  1.728568e-01  1.455425e-01  \n",
      "min   -3.864348e-01 -1.748698e-01 -3.611306e-01 -4.697296e-01  \n",
      "25%   -8.410918e-02 -5.689229e-02 -1.437393e-01 -1.363963e-01  \n",
      "50%   -6.589804e-03 -1.194847e-02 -2.779722e-02  3.027038e-02  \n",
      "75%    7.868151e-02  3.861333e-02  1.171303e-01  3.027038e-02  \n",
      "max    6.135652e-01  8.251302e-01  6.388694e-01  5.302704e-01  \n"
     ]
    }
   ],
   "source": [
    "num_features = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides',\n",
    "                'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH',\n",
    "                'sulphates', 'alcohol', 'quality']\n",
    " \n",
    "##scaled_features = {}\n",
    "scaled_wine_data = pd.DataFrame()\n",
    "for each in num_features:\n",
    "    mean, std = wine_data[each].mean(), wine_data[each].std(), \n",
    "    rng = np.max(wine_data[each]) - np.min(wine_data[each])\n",
    "    scaled_features[each] = [mean, std]\n",
    "    scaled_wine_data.loc[:, each] = (wine_data[each] - mean)/rng\n",
    "\n",
    "print(scaled_wine_data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now create the output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int64\n",
      "(6,)\n",
      "6497\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "output = wine_data['Class']\n",
    "print(output.dtypes)\n",
    "print(Y.shape)\n",
    "n_0, m = np.shape(scaled_wine_data)\n",
    "print(n_0)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (6497,) (6496,12) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-b4c3644e3367>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_grad_desc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscaled_wine_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_1\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m6496\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-103-44b6e38718c6>\u001b[0m in \u001b[0;36mrun_grad_desc\u001b[0;34m(num_epochs, learning_rate, X, Y, n_1)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mA1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfwd_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mB1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m                \u001b[0;31m# get predicted vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mback_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mB1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# get gradient and the cost from BP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mW1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mW1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dW1\"\u001b[0m\u001b[0;34m]\u001b[0m    \u001b[0;31m# update weight vector LR*gradient*[BP weights]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-102-639c39fed8cb>\u001b[0m in \u001b[0;36mback_prop\u001b[0;34m(A1, W1, bias, X, Y)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Cross entropy loss function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mA1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# cost of error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mdZ1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mY\u001b[0m                                            \u001b[0;31m# subtract actual from pred weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mdW1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdZ1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m                          \u001b[0;31m# calc new weight vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(left, right)\u001b[0m\n\u001b[1;32m   1067\u001b[0m             \u001b[0mrvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1069\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_na_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1070\u001b[0m         return construct_result(left, result,\n\u001b[1;32m   1071\u001b[0m                                 index=left.index, name=res_name, dtype=None)\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36msafe_na_op\u001b[0;34m(lvalues, rvalues)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mna_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mna_op\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1012\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpressions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr_rep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0meval_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/pandas/core/computation/expressions.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(op, op_str, a, b, use_numexpr, **eval_kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0muse_numexpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_numexpr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_bool_arith_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muse_numexpr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0meval_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/pandas/core/computation/expressions.py\u001b[0m in \u001b[0;36m_evaluate_numexpr\u001b[0;34m(op, op_str, a, b, truediv, reversed, **eval_kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_evaluate_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/pandas/core/computation/expressions.py\u001b[0m in \u001b[0;36m_evaluate_standard\u001b[0;34m(op, op_str, a, b, **eval_kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0m_store_test_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (6497,) (6496,12) "
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "params, loss_array = run_grad_desc(num_epochs,learning_rate,scaled_wine_data,output,n_1= 6496 )\n",
    "print(loss_array[num_epochs-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the boundary of separation is 0. That is values less than 0 belong to class 0 and greater than 0 belong to class 1.\n",
    "Key thing to note here is that the data we generated was a linearly separable data and hence there are many possible options for the separting plane. Unlike SVM, logistic regression does not necessarily find the best separting plane, but finds a locally optimum solution that separates the classes of data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of the loss array\n",
    "Here we want to ensure that the loss value per iteration is going down. However, note that the plot has not curved to reach stablizing value i.e we can run the algorithms more times to get a lower loss. However, this is not needed as the current value of the parameters can classify the input data accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (8.0, 6.0) #Set default plot sizes\n",
    "plt.rcParams['image.interpolation'] = 'nearest' #Use nearest neighbor for rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_array)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another example with handcrafted dataset\n",
    "values below 0.5 are assigned to class 1 and values above are set to class 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = np.array([[0.25,0.75],[0.1,0.9],[0.3,0.8],[0.8,0.25],[0.9,0.2],[0.7,0.1]])\n",
    "X = np.array([[0.11,0.12],[0.05,0.1],[0.15,0.11],[0.8,0.9],[0.9,0.8],[0.85,0.95]])\n",
    "X = X.T #Had to do this because, I did not declare the X array as (#dimension * # Datapoints)\n",
    "Y = np.array([1,1,1,0,0,0])\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params, loss_array = run_grad_desc(100000,0.01,X,Y,n_1= 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_array)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
