{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"blue\">Lesson 7 Performance Metrics & Hyperparameters</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE: Synthetic Minority Oversampling Technique\n",
    "For this lab, we'll use the bank_data.csv file that we've worked with in previous labs. \n",
    "\n",
    "Alert:  \n",
    "- Can be slow when data is large  \n",
    "- Can work on binary or multiclass classification data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# first install imblearn module\n",
    "pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import recall_score\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_df = pd.read_csv(\"https://library.startlearninglabs.uw.edu/DATASCI420/Datasets/Bank%20Data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Analysis\n",
    "Use the pandas_profiling library to perform a high-level exploratory analysis of your dataset. Explain your your observations for each variable in a summary below the profile. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for overall trends in the dataset\n",
    "import pandas_profiling\n",
    "pandas_profiling.ProfileReport(bank_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Analysis Summary\n",
    "This should describe your observations/questions/issues found for each variable. Outline your plan and reasoning for dealing with each observation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imbalance\n",
    "Let's check for imbalance in our dataset by looking at unique variable counts for a few features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_df.married.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_df.region.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_df.sex.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_df.mortgage.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize Observations on Imbalance\n",
    "Summarize what you found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encode Categorical Data\n",
    "We need to encode the categorical data before we can train and test models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of categorical variables\n",
    "bank_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store object column names in a list\n",
    "obj_cols = bank_df.select_dtypes(include=[\"object\"]).columns\n",
    "obj_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode your dataframe\n",
    "bank_df = pd.get_dummies(bank_df, columns=obj_cols, drop_first=True)\n",
    "bank_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into training and test sets\n",
    "We'll keep it simple and use PEP as a target again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the features from the targets\n",
    "targets = bank_df['pep_YES']\n",
    "features = bank_df.loc[:, bank_df.columns != \"pep_YES\"]\n",
    "\n",
    "# x is for features, y is for targets\n",
    "x_train, x_val, y_train, y_val = train_test_split(features, targets,\n",
    "                                                test_size = .2,\n",
    "                                                random_state=42)\n",
    "\n",
    "# x_val is the x_test data \n",
    "# y_val is the y_test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversample on Training Data\n",
    "If you oversample on both the training and test set, you will overfit your model. \n",
    "\n",
    "SMOTE creates synthetic observations of the minority class (bad candidates for PEP) by:\n",
    "\n",
    "1. Finding the k-nearest-neighbors for minority class observations (finding similar observations)  \n",
    "2. Randomly choosing one of the k-nearest-neighbors and using it to create a similar, but randomly tweaked, new observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace training_features and training_target with the correct names\n",
    "sm = SMOTE(random_state=12, ratio = 1.0)\n",
    "x_train_res, y_train_res = sm.fit_sample(training_features, training_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Random Forest Model\n",
    "Now that we've oversampled our dataset, let's see if it improves our random forest model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit the model\n",
    "clf_rf = RandomForestClassifier(n_estimators=25, random_state=12)\n",
    "clf_rf.fit(x_train_res, y_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Scores\n",
    "print('Validation Results')\n",
    "print(clf_rf.score(x_val, y_val))\n",
    "print(recall_score(y_val, clf_rf.predict(x_val)))\n",
    "\n",
    "# Replace test_features and test_target with the correct names\n",
    "print('\\nTest Results')\n",
    "print(clf_rf.score(test_features, test_target))\n",
    "print(recall_score(test_target, clf_rf.predict(test_features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation results should closely match the unseen test data results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
