{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"blue\">Lesson 3 Basic Models</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Classification Algorithms on the Same Dataset\n",
    "\n",
    "I'm using a KFold validation on the Pima Indian Tribe Diabetes Dataset (downloaded from Kaggle) with several classification algorithms. The Pima indians have a prevalence for diabetes due to a change from their traditional diet (and dancing--they happen to like polka dancing). The test is binary (diabetes == true or diabetes == false).\n",
    "\n",
    "Don't worry about the \"details\" for now. Just focus on the testing of the accuracy of these side by side. This is a method you can use to identify the best classifier for a particular type of problem.\n",
    "\n",
    "**Linear**\n",
    "* Logistic Regression Classification <--yes you can do this!\n",
    "* Linear Discriminant Analysis (or LDA)\n",
    "\n",
    "**Nonlinear**\n",
    "* kNN Classifier\n",
    "* Naive Bayes Classifier\n",
    "* Classification tree \n",
    "* Support Vector Classifier (SVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data file\n",
    "import pandas as pd\n",
    " \n",
    "data = pd.read_csv(\"https://library.startlearninglabs.uw.edu/DATASCI420/Datasets/diabetes.csv\", header=0) \n",
    "\n",
    "X = data.iloc[:, 0:8]   # load features into X DF\n",
    "Y = data.iloc[:, 8]     # Load target into Y DF\n",
    "\n",
    "# Add with viewing the data\n",
    "pd.set_option('display.width', 100) \n",
    "pd.set_option('precision', 2)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "\n",
    "data.plot(kind='box', subplots=True, layout=(5,3), sharex=False, sharey=False, figsize=(12,15))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear ML Algorithms\n",
    "\n",
    "### Logistic Regression Classification\n",
    "Assumes a normal distribution for the independent variables and models binary classification problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=7)  # 10 fold cross validation ; \n",
    "                                            # 7 random state is to assure consistent results\n",
    "log_reg_results = cross_val_score(LogisticRegression(), X, Y, cv=kfold, scoring='accuracy') \n",
    "print(\"Accuracy-> mean:%.3f%% (std:%.3f)\" % (log_reg_results.mean()*100, log_reg_results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis\n",
    "Binary and mulit-class classification. Also assumes a normal distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "lda_results = cross_val_score(LinearDiscriminantAnalysis(), X, Y, cv=kfold) \n",
    "print(\"Accuracy: %.3f%% (std:%.3f)\" % (lda_results.mean()*100, lda_results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linear ML Algorithms\n",
    "### k-Nearest Neighbors (kNN) \n",
    "kNN's use a distance metric to find the k most similar inputs in the training data for a new instance. The *mean outcome* of the neighbors is the prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "kNN_results = cross_val_score(KNeighborsClassifier(), X, Y, cv=kfold) \n",
    "print(\"Accuracy-> mean:%.3f%% (std:%.3f)\" % (kNN_results.mean()*100, kNN_results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "Calculates the probability of each class and the conditional probability of each class given an input value. These probabilities are estimated for new data and multiplied together. The reason it is called *Naive* is that it assumes that all of the inputs are independent. When working with real-valued data, a Gaussian distribution is assumed to easily estimate the probabilities for input variables using the Gaussian Probability Density Function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nbc_results = cross_val_score(GaussianNB(), X, Y, cv=kfold)\n",
    "print(\"Accuracy: %.3f%% (std:%.3f)\" % (nbc_results.mean()*100, nbc_results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification and Regression Trees (CART)\n",
    "These ML algorithms construct a binary tree from the training data. Split points are decided greedily, evaluating each attribute in order to minimize the cost function (default is Gini index). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "CART_results = cross_val_score(DecisionTreeClassifier(), X, Y, cv=kfold)\n",
    "print(\"Accuracy-> mean:%.3f%% (std:%.3f)\" % (CART_results.mean()*100, CART_results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines\n",
    "Creates the best possible (most optimal) division between two classes of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc_results = cross_val_score(SVC(), X, Y, cv=kfold)\n",
    "print(\"Accuracy: %.3f%% (std:%.3f)\" % (svc_results.mean()*100, svc_results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LogReg Accuracy-> %.3f%% (std:%.3f)\" % (log_reg_results.mean()*100, log_reg_results.std()))\n",
    "print(\"LDA    Accuracy-> %.3f%% (std:%.3f)\" % (lda_results.mean()*100, lda_results.std()))\n",
    "print(\"kNN    Accuracy-> %.3f%% (std:%.3f)\" % (kNN_results.mean()*100, kNN_results.std()))\n",
    "print(\"NBC    Accuracy-> %.3f%% (std:%.3f)\" % (nbc_results.mean()*100, nbc_results.std()))\n",
    "print(\"CART   Accuracy-> %.3f%% (std:%.3f)\" % (CART_results.mean()*100, CART_results.std()))\n",
    "print(\"SVC    Accuracy-> %.3f%% (std:%.3f)\" % (svc_results.mean()*100, svc_results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this tell us about the dataset?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
